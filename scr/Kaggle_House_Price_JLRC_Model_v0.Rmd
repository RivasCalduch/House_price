---
title: "Kaggle_House_Price_Model_V0_2"
author: "Jose Luis Rivas Calduch"
date: "9 de febrero de 2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

```{r, echo=FALSE, include=FALSE}

#Cargamos las diferentes librerias a emplear

#Lectura del fichero csv
library(readr)

# Manejo de data frames
library(tidyverse)

#Fusion de graficos
library(gridExtra)

#Graficos
library(ggplot2)

#Arboles
library(tree)
library(rpart)

#Modificar nombres data set
library(reshape)

library(MASS)

# Algoritmo randomForest
library(randomForest)

library(dplyr)

```


```{r}

#Lectura de data frame de entrenamiento
#modelTrain <- read_csv("modelTrainScale.csv")
modelTrain <- read_csv("modelTrain.csv")

#glimpse(modelTrain)

modelTrain = rename(modelTrain, c(`1stFlrSF` = 'FirstststFlrSF'))
#modelTrain = rename(modelTrain, c(`1stFlrSF_scale` = 'FirstststFlrSF_scale'))

#Convertimos en factor todos los campos caracter
character_vars <- lapply(modelTrain, class) == "character"

modelTrain[, character_vars] <- lapply(modelTrain[, character_vars], as.factor)

glimpse(modelTrain)

```

```{r}
# Verificamos si existen columnas con NAs
colSums(is.na(modelTrain))

```

### Dividimos el data set para poder hacer testing

```{r}

set.seed(123)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(modelTrain))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(modelTrain)), size = smp_size)

datos_train <- modelTrain[train_ind, ]
datos_test <- modelTrain[-train_ind, ]

```

```{r}
# Verificamos si existen columnas con NAs
colSums(is.na(datos_train))

```


### Modelo de regresion multiple

Entrenamiento del modelo con las variables seleccionadas tras el analisis.

```{r}

modelRM <- lm(averagePrice_surface ~
                BldgType + 
                MasVnrType_clean + 
                MasVnrArea_clean + 
                ExterQual + 
                CentralAir + 
                FirstststFlrSF+ 
                GrLivArea + 
                KitchenQual_clean + 
                GarageArea_clean +
                PavedDrive +
                WoodDeckSF +
                OpenPorchSF +
                MSSubClass_agrup +
                MSZoning_agrup +
                Condition1 +
                HouseStyle +
                antiguedad +
                reforma,
              data = datos_train)

summary(modelRM)

```

modelRM <- lm(averagePrice_surface ~
                BldgType + 
                MasVnrType_clean + 
                MasVnrArea_clean_scale + 
                ExterQual + 
                CentralAir + 
                FirstststFlrSF_scale+ 
                GrLivArea_scale + 
                KitchenQual_clean + 
                GarageArea_clean_scale +
                PavedDrive +
                WoodDeckSF_scale +
                OpenPorchSF_scale +
                MSSubClass_agrup +
                MSZoning_agrup +
                Condition1 +
                HouseStyle +
                antiguedad_scale +
                reforma_scale,
              data = datos_train)

summary(modelRM)

```{r}

confint(modelRM)

```

#### Test del modelo

```{r}

test.modelRM <- predict(modelRM, datos_test)

test_mse <- mean((test.modelRM - datos_test$averagePrice_surface)^2)

test_rmse <- sqrt(test_mse)

print(paste("MSE =",test_mse))

print(paste("RMSE =",test_rmse))

```

Entrenamiento del modelo con las variables seleccionadas tras el analisis con todos los datos de entrenamiento.

```{r}

modelRMcompleto <- lm(averagePrice_surface ~
                BldgType + 
                MasVnrType_clean + 
                MasVnrArea_clean_scale + 
                ExterQual + 
                CentralAir + 
                FirstststFlrSF_scale+ 
                GrLivArea_scale + 
                KitchenQual_clean + 
                GarageArea_clean_scale +
                PavedDrive +
                WoodDeckSF_scale +
                OpenPorchSF_scale +
                MSSubClass_agrup +
                MSZoning_agrup +
                Condition1 +
                HouseStyle +
                antiguedad_scale +
                reforma_scale,
              data = modelTrain)

summary(modelRMcompleto)

```


### Modelo de arbol de regresión

```{r}

#Convertimos en factor todos los campos caracter
#character_vars <- lapply(modelTrain, class) == "character"

#modelTrain[, character_vars] <- lapply(modelTrain[, character_vars], as.factor)

#glimpse(modelTrain)

set.seed(123)

modelTree <- tree(averagePrice_surface ~
                BldgType + 
                MasVnrType_clean + 
                MasVnrArea_clean_scale + 
                ExterQual + 
                CentralAir + 
                FirstststFlrSF_scale+ 
                GrLivArea_scale + 
                KitchenQual_clean + 
                GarageArea_clean_scale +
                PavedDrive +
                WoodDeckSF_scale +
                OpenPorchSF_scale +
                MSSubClass_agrup +
                MSZoning_agrup +
                Condition1 +
                HouseStyle +
                antiguedad_scale +
                reforma_scale,
                data = datos_train)

modelTree

```

```{r}

set.seed(3)
cv_arbol <- cv.tree(modelTree, K = 10)
cv_arbol

```

```{r}
library(ggplot2)
library(ggpubr)

resultados_cv <- data.frame(n_nodos = cv_arbol$size, deviance = cv_arbol$dev,
                            alpha = cv_arbol$k)
p1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs tamaño del árbol") + theme_bw() 
  

p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs hiperparámetro alpha") + theme_bw() 

ggarrange(p1, p2)
```

```{r}

arbol_pruning <- prune.tree(tree = modelTree, best = 12)

```

#### Test del modelo

```{r}

test.arbol_pruning <- predict(arbol_pruning, datos_test)

test_mse <- mean((test.arbol_pruning - datos_test$averagePrice_surface)^2)

test_rmse <- sqrt(test_mse)

paste("MSE =",test_mse)

paste("RMSE =",test_rmse)

```

### Modelo random forest

```{r}
# Evaluación del número de predictores

tuning_rf_mtry <- function(df, y, ntree = 500){
  # Esta función devuelve el out-of-bag-MSE de un modelo RandomForest en función
  # del número de predictores evaluados (mtry)
  
  # Argumentos:
  #   df = data frame con los predictores y variable respuesta
  #   y  = nombre de la variable respuesta
  #   ntree = número de árboles creados en el modelo randomForest

  require(dplyr)
  max_predictores <- ncol(df) - 1
  n_predictores   <- rep(NA, max_predictores)
  oob_mse         <- rep(NA, max_predictores)
  for (i in 1:max_predictores) {
    set.seed(123)
    f <- formula(paste(y,"~ ."))
    modelo_rf <- randomForest(formula = f, data = df, mtry = i, ntree = ntree)
    n_predictores[i] <- i
    oob_mse[i] <- tail(modelo_rf$mse, n = 1)
  }
  results <- data.frame(n_predictores, oob_mse)
  return(results)
}

hiperparametro_mtry <-  tuning_rf_mtry(df = modelTrain, y = "averagePrice_surface")
hiperparametro_mtry %>% arrange(oob_mse) %>% head()

```

```{r}

ggplot(data = hiperparametro_mtry, aes(x = n_predictores, y = oob_mse)) +
  scale_x_continuous(breaks = hiperparametro_mtry$n_predictores) +
  geom_line() +
  geom_point() +
  geom_point(data = hiperparametro_mtry %>% arrange(oob_mse) %>% head(1),
             color = "red") +
  labs(title = "Evolución del out-of-bag-error vs mtry",
       x = "nº predictores empleados") +
  theme_bw()

```

```{r}

tuning_rf_nodesize <- function(df, y, size = NULL, ntree = 500){
  # Esta función devuelve el out-of-bag-MSE de un modelo randomForest en función
  # del tamaño mínimo de los nodos terminales (nodesize).
  
  # Argumentos:
  #   df = data frame con los predictores y variable respuesta
  #   y  = nombre de la variable respuesta
  #   sizes = tamaños evaluados
  #   ntree = número de árboles creados en el modelo randomForest

  require(dplyr)
  if (is.null(size)){
    size <- seq(from = 1, to = nrow(df), by = 5)
  }
  oob_mse <- rep(NA, length(size))
  for (i in seq_along(size)) {
    set.seed(123)
    f <- formula(paste(y,"~ ."))
    modelo_rf <- randomForest(formula = f, data = df, mtry = 5, ntree = ntree,
                              nodesize = i)
    oob_mse[i] <- tail(modelo_rf$mse, n = 1)
  }
  results <- data.frame(size, oob_mse)
  return(results)
}

hiperparametro_nodesize <-  tuning_rf_nodesize(df = modelTrain, y = "averagePrice_surface",
                                               size = c(1:20))
hiperparametro_nodesize %>% arrange(oob_mse) %>% head()

```

```{r}

ggplot(data = hiperparametro_nodesize, aes(x = size, y = oob_mse)) +
  scale_x_continuous(breaks = hiperparametro_nodesize$size) +
  geom_line() +
  geom_point() +
  geom_point(data = hiperparametro_nodesize %>% arrange(oob_mse) %>% head(1),
             color = "red") +
  labs(title = "Evolución del out-of-bag-error vs nodesize",
       x = "nº observaciones en nodos terminales") +
  theme_bw()

```

```{r}

modelo_randomforest <- randomForest(averagePrice_surface ~ 
                                    BldgType + 
                                    MasVnrType_clean + 
                                    MasVnrArea_clean_scale + 
                                    ExterQual + 
                                    CentralAir + 
                                    FirstststFlrSF_scale+ 
                                    GrLivArea_scale + 
                                    KitchenQual_clean + 
                                    GarageArea_clean_scale +
                                    PavedDrive +
                                    WoodDeckSF_scale +
                                    OpenPorchSF_scale +
                                    MSSubClass_agrup +
                                    MSZoning_agrup +
                                    Condition1 +
                                    HouseStyle +
                                    antiguedad_scale +
                                    reforma_scale,
                                    data = datos_train,
                                    mtry = 8 , ntree = 500, nodesize = 6,
                                    importance = TRUE)
oob_mse <- data.frame(oob_mse = modelo_randomforest$mse,
                      arboles = seq_along(modelo_randomforest$mse))
ggplot(data = oob_mse, aes(x = arboles, y = oob_mse )) +
  geom_line() +
  labs(title = "Evolución del out-of-bag-error vs número árboles",
       x = "nº árboles") +
  theme_bw()

```

```{r}

set.seed(123)
modelo_randomforest <- randomForest(averagePrice_surface ~ 
                                    BldgType + 
                                    MasVnrType_clean + 
                                    MasVnrArea_clean_scale + 
                                    ExterQual + 
                                    CentralAir + 
                                    FirstststFlrSF_scale+ 
                                    GrLivArea_scale + 
                                    KitchenQual_clean + 
                                    GarageArea_clean_scale +
                                    PavedDrive +
                                    WoodDeckSF_scale +
                                    OpenPorchSF_scale +
                                    MSSubClass_agrup +
                                    MSZoning_agrup +
                                    Condition1 +
                                    HouseStyle +
                                    antiguedad_scale +
                                    reforma_scale, 
                                    data = datos_train,
                                    mtry = 8 , ntree = 500, nodesize = 6,
                                    importance = TRUE)
modelo_randomforest

```

#### Test del modelo

```{r}

test.randomforest <- predict(modelo_randomforest, datos_test)

test_mse <- mean((test.randomforest - datos_test$averagePrice_surface)^2)

test_rmse <- sqrt(test_mse)

print(paste("MSE =",test_mse))

print(paste("RMSE =",test_rmse))

```

```{r}

#Empleamos todas las observaciones para mejorar el modelo


set.seed(123)
modelo_randomforest <- randomForest(averagePrice_surface ~ 
                                    BldgType + 
                                    MasVnrType_clean + 
                                    MasVnrArea_clean_scale + 
                                    ExterQual + 
                                    CentralAir + 
                                    FirstststFlrSF_scale+ 
                                    GrLivArea_scale + 
                                    KitchenQual_clean + 
                                    GarageArea_clean_scale +
                                    PavedDrive +
                                    WoodDeckSF_scale +
                                    OpenPorchSF_scale +
                                    MSSubClass_agrup +
                                    MSZoning_agrup +
                                    Condition1 +
                                    HouseStyle +
                                    antiguedad_scale +
                                    reforma_scale,
                                    data = modelTrain,
                                    mtry = 4 , ntree = 300, nodesize = 7,
                                    importance = TRUE)
modelo_randomforest

```

```{r}

library(tidyverse)
library(ggpubr)
importancia_pred <- as.data.frame(importance(modelo_randomforest, scale = TRUE))
importancia_pred <- rownames_to_column(importancia_pred, var = "variable")
p1 <- ggplot(data = importancia_pred, aes(x = reorder(variable, `%IncMSE`),
                                          y = `%IncMSE`,
                                          fill = `%IncMSE`)) +
    labs(x = "variable", title = "Reducción de MSE") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")

p2 <- ggplot(data = importancia_pred, aes(x = reorder(variable, IncNodePurity),
                                          y = IncNodePurity,
                                          fill = IncNodePurity)) +
    labs(x = "variable", title = "Reducción de pureza") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")
ggarrange(p1, p2)

```



#Prediccion de los datos de test.

```{r}

#Lectura de data frame de test

modelTest <- read_csv("modelTest.csv")

modelTest = rename(modelTest, c(`1stFlrSF` = 'FirstststFlrSF'))

#glimpse(modelTest)

modelTestScale <- read_csv("modelTestScale.csv")

modelTestScale = rename(modelTestScale, c(`1stFlrSF_scale` = 'FirstststFlrSF_scale'))
modelTest = rename(modelTest, c(`1stFlrSF` = 'FirstststFlrSF'))

#Convertimos en factor todos los campos caracter
character_vars <- lapply(modelTestScale, class) == "character"
character_vars <- lapply(modelTest, class) == "character"

modelTestScale[, character_vars] <- lapply(modelTestScale[, character_vars], as.factor)
modelTest[, character_vars] <- lapply(modelTest[, character_vars], as.factor)

glimpse(modelTestScale)

```

```{r}

# Ajuste de niveles

levels(modelTestScale$HouseStyle) <- levels(modelTrain$HouseStyle)
levels(modelTest$HouseStyle) <- levels(modelTrain$HouseStyle)

```


```{r}

p.modelRM <- predict(modelRM, modelTest)

#p.modelRM <- predict(modelRMcompleto, modelTestScale)

#p.modelTree <- predict(arbol_pruning, modelTestScale)

#p.modelRF <- predict(modelo_randomforest, modelTestScale)

```

Unimos el resultado al data set de test

```{r}

result <- data.frame(modelTest,p.modelRM)

```

Creamos la variable dependiente a evaluar.

```{r}

result$SalePrice <- result$GrLivArea * result$p.modelRM

```

Creamos la salida

```{r}

kaggle2 <- result[,c("Id","SalePrice")]

```


```{r}

#Creamos el set a enviar

write.csv(kaggle2, file = "submissionV02_16.csv", row.names = F)

```





